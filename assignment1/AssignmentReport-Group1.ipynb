{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "**By Jonas Halle and Alexander Rambech**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that: $\\frac{\\partial C^n(w)}{\\partial w_i} = \\frac{\\partial C^n(w)}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial w_i} = \\frac{\\partial C^n}{\\partial f(x^n)} \\frac{\\partial f(x^n)}{\\partial w_i}$, with partial derivatives given by: \n",
    "\n",
    "  * $\\frac{\\partial f(x^n)}{\\partial w_i} = x^n_i f(x^n)(1 - f(x^n))$\n",
    "\n",
    "  * $\\frac{\\partial C^n}{\\partial f(x^n)} = -(\\frac{y^n}{\\hat{y}^n} - \\frac{1 - y^n}{1 - \\hat{y}_n})$\n",
    "\n",
    "Combining these two terms results in:\n",
    "\n",
    "$\\frac{\\partial C^n(w)}{\\partial w_i} = -(\\frac{y^n}{\\hat{y}^n} - \\frac{1 - y^n}{1 - \\hat{y}_n})~x^n_i ~f(x^n)(1 - f(x^n)) = -(y^n - y^n\\hat{y}^n - \\hat{y} + y^n\\hat{y}_n)x_i = -(y^n - \\hat{y}^n)x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "\n",
    "We find the gradient of the cross entropy loss for the softmax function with regards to the weights using $\\frac{\\partial C^n(w)}{\\partial w_{kj}} = \\frac{\\partial C^n}{\\partial \\hat{y}^n_k} \\frac{\\partial \\hat{y}^n_k}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}}$, where $C^n(w) = -\\sum_{k=1}^{K}y^n_k~ln(\\hat{y}_k^n)$, $n \\in N$ is the current sample and $k \\in K$ is the node index of the output layer.\n",
    "\n",
    "We start by solving the partial derivatives:\n",
    "\n",
    "  * $\\frac{\\partial C^n}{\\partial \\hat{y}^n_k} = -\\sum_{k=1}^K \\frac{y_k^n}{\\hat{y}^n_k} = - \\frac{1}{\\hat{y}_k^n}$, where $\\sum_{k=1}^{K} y_k^n = 1$\n",
    "  \n",
    "  * $\\frac{\\partial \\hat{y}^n_k}{\\partial z_k} = \\frac{\\partial}{\\partial z_k}\\left(\\frac{e^{z_k}}{\\sum_{k'}^K e^{z_{k'}}}\\right) = \\frac{\\sum_{k'}^K e^{z_{k'}} \\frac{\\partial e^{z_k}}{\\partial z_k} - \\frac{\\partial}{\\partial z_k} \\left[\\sum_{k'}^K e^{z_{k'}}\\right] e^{z_k}}{\\left[ \\sum_{k'}^K e^{z_{k'}} \\right]^2} = \\frac{\\sum_{k'}^K e^{z_{k'}} e^{z_k} - e^{z_k} e^{z_k}}{\\left[ \\sum_{k'}^K e^{z_{k'}} \\right]^2} = \\frac{e^{z_k}}{\\sum_{k'}^K e^{z_{k'}}} \\cdot \\frac{\\sum_{k'}^K e^{z_{k'}}- e^{z_k}}{\\sum_{k'}^K e^{z_{k'}}} = \\hat{y}_k^n (1-\\hat{y}_k^n)$, here the qoutient rule is used with $\\hat{y}_k = \\frac{e^{z_k}}{\\sum_{k'}^K e^{z_{k'}}}$\n",
    "  \n",
    "  * $\\frac{\\partial z_k}{\\partial w_{jk}} = \\frac{\\partial}{\\partial w_{jk}} (x_jw_{jk} + b_k) = x_j$\n",
    "  \n",
    "This is then multiplied together:\n",
    "  * $\\frac{\\partial C^n(w)}{\\partial w_{kj}} = \\frac{\\partial C^n}{\\partial \\hat{y}^n_k} \\frac{\\partial \\hat{y}^n_k}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}} = - \\frac{1}{\\hat{y}_k^n}\\hat{y}_k^n(1 - \\hat{y}_k^n)x_j = - x_j(1 - \\hat{y}_k^n)$, this is not correct but it was the closest we got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def pre_process_images(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 784] in the range (0, 255)\n",
    "    Returns:\n",
    "        X: images of shape [batch size, 785] in the range (-1, 1)\n",
    "    \"\"\"\n",
    "    assert X.shape[1] == 784,\\\n",
    "        f\"X.shape[1]: {X.shape[1]}, should be 784\"\n",
    "    # TODO implement this function (Task 2a)\n",
    "    batch_size = X.shape[0]\n",
    "    lfunc = lambda e: -1+e*2/255\n",
    "    vfunc = np.vectorize(lfunc)\n",
    "    X = vfunc(X)\n",
    "    bias_trick = np.ones((batch_size, 1))\n",
    "    X = np.hstack((bias_trick, X))\n",
    "    return X\n",
    "\n",
    "\n",
    "def cross_entropy_loss(targets: np.ndarray, outputs: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        targets: labels/targets of each image of shape: [batch size, 1]\n",
    "        outputs: outputs of model of shape: [batch size, 1]\n",
    "    Returns:\n",
    "        Cross entropy error (float)\n",
    "    \"\"\"\n",
    "    # TODO implement this function (Task 2a)\n",
    "    \n",
    "\n",
    "\n",
    "    assert targets.shape == outputs.shape,\\\n",
    "        f\"Targets shape: {targets.shape}, outputs: {outputs.shape}\"\n",
    "    \n",
    "    N = targets.shape[0]\n",
    "    C_n = -(targets.T.dot(np.log(outputs)) + (1-targets).T.dot(np.log(1-outputs)))\n",
    "    loss = 1/N*np.sum(C_n)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "class BinaryModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Define number of input nodes\n",
    "        self.I = 785\n",
    "        self.w = np.zeros((self.I, 1))\n",
    "        self.grad = None\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "        Returns:\n",
    "            y: output of model with shape [batch size, 1]\n",
    "        \"\"\"\n",
    "        # TODO implement this function (Task 2a)\n",
    "    \n",
    "    \n",
    "        y = 1/(1+np.exp(-X.dot(self.w)))\n",
    "        \n",
    "        return y\n",
    "        #return None\n",
    "\n",
    "    def backward(self, X: np.ndarray, outputs: np.ndarray, targets: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Computes the gradient and saves it to the variable self.grad\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "            outputs: outputs of model of shape: [batch size, 1]\n",
    "            targets: labels/targets of each image of shape: [batch size, 1]\n",
    "        \"\"\"\n",
    "        # TODO implement this function (Task 2a)\n",
    "        \n",
    "        assert targets.shape == outputs.shape,\\\n",
    "            f\"Output shape: {outputs.shape}, targets: {targets.shape}\"\n",
    "        self.grad = np.zeros_like(self.w)\n",
    "        assert self.grad.shape == self.w.shape,\\\n",
    "            f\"Grad shape: {self.grad.shape}, w: {self.w.shape}\"\n",
    "        self.grad = -(targets - outputs).T.dot(X)/X.shape[0]\n",
    "        self.grad = self.grad.T\n",
    "        \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "def gradient_approximation_test(model: BinaryModel, X: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "        Numerical approximation for gradients. Should not be edited. \n",
    "        Details about this test is given in the appendix in the assignment.\n",
    "    \"\"\"\n",
    "    w_orig = np.random.normal(loc=0, scale=1/model.w.shape[0]**2, size=model.w.shape)\n",
    "    epsilon = 1e-3\n",
    "    for i in range(w_orig.shape[0]):\n",
    "        model.w = w_orig.copy()\n",
    "        orig = w_orig[i].copy()\n",
    "        model.w[i] = orig + epsilon\n",
    "        logits = model.forward(X)\n",
    "        cost1 = cross_entropy_loss(Y, logits)\n",
    "        model.w[i] = orig - epsilon\n",
    "        logits = model.forward(X)\n",
    "        cost2 = cross_entropy_loss(Y, logits)\n",
    "        gradient_approximation = (cost1 - cost2) / (2 * epsilon)\n",
    "        model.w[i] = orig\n",
    "        # Actual gradient\n",
    "        logits = model.forward(X)\n",
    "        model.backward(X, logits, Y)\n",
    "        difference = gradient_approximation - model.grad[i, 0]\n",
    "        assert abs(difference) <= epsilon**2,\\\n",
    "            f\"Calculated gradient is incorrect. \" \\\n",
    "            f\"Approximation: {gradient_approximation}, actual gradient: {model.grad[i,0]}\\n\" \\\n",
    "            f\"If this test fails there could be errors in your cross entropy loss function, \" \\\n",
    "            f\"forward function or backward function\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    category1, category2 = 2, 3\n",
    "    X_train, Y_train, *_ = utils.load_binary_dataset(category1, category2)\n",
    "    X_train = pre_process_images(X_train)\n",
    "    assert X_train.max() <= 1.0, f\"The images (X_train) should be normalized to the range [-1, 1]\"\n",
    "    assert X_train.min() < 0 and X_train.min() >= -1, f\"The images (X_train) should be normalized to the range [-1, 1]\"\n",
    "    assert X_train.shape[1] == 785,\\\n",
    "        f\"Expected X_train to have 785 elements per image. Shape was: {X_train.shape}\"\n",
    "\n",
    "    # Simple test for forward pass. Note that this does not cover all errors!\n",
    "    model = BinaryModel()\n",
    "    logits = model.forward(X_train)\n",
    "    np.testing.assert_almost_equal(\n",
    "        logits.mean(), .5,\n",
    "        err_msg=\"Since the weights are all 0's, the sigmoid activation should be 0.5\")\n",
    "\n",
    "    # Gradient approximation check for 100 images\n",
    "    X_train = X_train[:100]\n",
    "    Y_train = Y_train[:100]\n",
    "    for i in range(2):\n",
    "        gradient_approximation_test(model, X_train, Y_train)\n",
    "        model.w = np.random.randn(*model.w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task2.py\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from task2a import cross_entropy_loss, BinaryModel, pre_process_images\n",
    "from trainer import BaseTrainer\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def calculate_accuracy(X: np.ndarray, targets: np.ndarray, model: BinaryModel) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 785]\n",
    "        targets: labels/targets of each image of shape: [batch size, 1]\n",
    "        model: model of class BinaryModel\n",
    "    Returns:\n",
    "        Accuracy (float)\n",
    "    \"\"\"\n",
    "    # TODO Implement this function (Task 2c)\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class LogisticTrainer(BaseTrainer):\n",
    "\n",
    "    def train_step(self, X_batch: np.ndarray, Y_batch: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform forward, backward and gradient descent step here.\n",
    "        The function is called once for every batch (see trainer.py) to perform the train step.\n",
    "        The function returns the mean loss value which is then automatically logged in our variable self.train_history.\n",
    "\n",
    "        Args:\n",
    "            X: one batch of images\n",
    "            Y: one batch of labels\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function (task 2b)\n",
    "        outputs = model.forward(X_batch)                \n",
    "        loss = cross_entropy_loss(Y_batch, outputs)\n",
    "        model.backward(X_batch,outputs,Y_batch)\n",
    "        model.w = model.w-learning_rate*model.grad\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self):\n",
    "        \"\"\"\n",
    "        Perform a validation step to evaluate the model at the current step for the validation set.\n",
    "        Also calculates the current accuracy of the model on the train set.\n",
    "        Returns:\n",
    "            loss (float): cross entropy loss over the whole dataset\n",
    "            accuracy_ (float): accuracy over the whole dataset\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "            accuracy_train (float): Accuracy on train dataset\n",
    "            accuracy_val (float): Accuracy on the validation dataset\n",
    "        \"\"\"\n",
    "        # NO NEED TO CHANGE THIS FUNCTION\n",
    "        logits = self.model.forward(self.X_val)\n",
    "        loss = cross_entropy_loss(Y_val, logits)\n",
    "\n",
    "        accuracy_train = calculate_accuracy(\n",
    "            X_train, Y_train, self.model)\n",
    "        accuracy_val = calculate_accuracy(\n",
    "            X_val, Y_val, self.model)\n",
    "        return loss, accuracy_train, accuracy_val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # hyperparameters DO NOT CHANGE IF NOT SPECIFIED IN ASSIGNMENT TEXT\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.05\n",
    "    batch_size = 128\n",
    "    shuffle_dataset = False\n",
    "\n",
    "    # Load dataset\n",
    "    category1, category2 = 2, 3\n",
    "    X_train, Y_train, X_val, Y_val = utils.load_binary_dataset(\n",
    "        category1, category2)\n",
    "\n",
    "    X_train = pre_process_images(X_train)\n",
    "    X_val = pre_process_images(X_val)\n",
    "\n",
    "    # ANY PARTS OF THE CODE BELOW THIS CAN BE CHANGED.\n",
    "\n",
    "    # Intialize model\n",
    "    model = BinaryModel()\n",
    "    # Train model\n",
    "    trainer = LogisticTrainer(\n",
    "        model, learning_rate, batch_size, shuffle_dataset,\n",
    "        X_train, Y_train, X_val, Y_val,\n",
    "    )\n",
    "    train_history, val_history = trainer.train(num_epochs)\n",
    "\n",
    "    # Plot and print everything you want of information\n",
    "\n",
    "    print(\"Final Train Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_train, model.forward(X_train)))\n",
    "    print(\"Final Validation Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_val, model.forward(X_val)))\n",
    "    print(\"Train accuracy:\", calculate_accuracy(X_train, Y_train, model))\n",
    "    print(\"Validation accuracy:\", calculate_accuracy(X_val, Y_val, model))\n",
    "\n",
    "    # Plot loss for first model (task 2b)\n",
    "    plt.ylim([0., .2])\n",
    "    utils.plot_loss(train_history[\"loss\"],\n",
    "                    \"Training Loss\", npoints_to_average=10)\n",
    "    utils.plot_loss(val_history[\"loss\"], \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Cross Entropy Loss - Average\")\n",
    "    plt.savefig(\"task2b_binary_train_loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task2.py\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from task2a import cross_entropy_loss, BinaryModel, pre_process_images\n",
    "from trainer import BaseTrainer\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def calculate_accuracy(X: np.ndarray, targets: np.ndarray, model: BinaryModel) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 785]\n",
    "        targets: labels/targets of each image of shape: [batch size, 1]\n",
    "        model: model of class BinaryModel\n",
    "    Returns:\n",
    "        Accuracy (float)\n",
    "    \"\"\"\n",
    "    # TODO Implement this function (Task 2c)\n",
    "    accuracy = 0.0\n",
    "    correct_predictions = 0\n",
    "    predictions = targets.shape[0]\n",
    "    outputs = model.forward(X)\n",
    "    \n",
    "    for idx, val in enumerate(outputs):\n",
    "        target = targets[idx]\n",
    "        if val >= 0.5 and target == 1 or val<0.5 and target == 0:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions/predictions\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class LogisticTrainer(BaseTrainer):\n",
    "\n",
    "    def train_step(self, X_batch: np.ndarray, Y_batch: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform forward, backward and gradient descent step here.\n",
    "        The function is called once for every batch (see trainer.py) to perform the train step.\n",
    "        The function returns the mean loss value which is then automatically logged in our variable self.train_history.\n",
    "\n",
    "        Args:\n",
    "            X: one batch of images\n",
    "            Y: one batch of labels\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function (task 2b)\n",
    "        outputs = model.forward(X_batch)                \n",
    "        loss = cross_entropy_loss(Y_batch, outputs)\n",
    "        model.backward(X_batch,outputs,Y_batch)\n",
    "        model.w = model.w-learning_rate*model.grad\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self):\n",
    "        \"\"\"\n",
    "        Perform a validation step to evaluate the model at the current step for the validation set.\n",
    "        Also calculates the current accuracy of the model on the train set.\n",
    "        Returns:\n",
    "            loss (float): cross entropy loss over the whole dataset\n",
    "            accuracy_ (float): accuracy over the whole dataset\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "            accuracy_train (float): Accuracy on train dataset\n",
    "            accuracy_val (float): Accuracy on the validation dataset\n",
    "        \"\"\"\n",
    "        # NO NEED TO CHANGE THIS FUNCTION\n",
    "        logits = self.model.forward(self.X_val)\n",
    "        loss = cross_entropy_loss(Y_val, logits)\n",
    "\n",
    "        accuracy_train = calculate_accuracy(\n",
    "            X_train, Y_train, self.model)\n",
    "        accuracy_val = calculate_accuracy(\n",
    "            X_val, Y_val, self.model)\n",
    "        return loss, accuracy_train, accuracy_val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # hyperparameters DO NOT CHANGE IF NOT SPECIFIED IN ASSIGNMENT TEXT\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.05\n",
    "    batch_size = 128\n",
    "    shuffle_dataset = False\n",
    "\n",
    "    # Load dataset\n",
    "    category1, category2 = 2, 3\n",
    "    X_train, Y_train, X_val, Y_val = utils.load_binary_dataset(\n",
    "        category1, category2)\n",
    "\n",
    "    X_train = pre_process_images(X_train)\n",
    "    X_val = pre_process_images(X_val)\n",
    "\n",
    "    # ANY PARTS OF THE CODE BELOW THIS CAN BE CHANGED.\n",
    "\n",
    "    # Intialize model\n",
    "    model = BinaryModel()\n",
    "    # Train model\n",
    "    trainer = LogisticTrainer(\n",
    "        model, learning_rate, batch_size, shuffle_dataset,\n",
    "        X_train, Y_train, X_val, Y_val,\n",
    "    )\n",
    "    train_history, val_history = trainer.train(num_epochs)\n",
    "\n",
    "    # Plot and print everything you want of information\n",
    "\n",
    "    print(\"Final Train Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_train, model.forward(X_train)))\n",
    "    print(\"Final Validation Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_val, model.forward(X_val)))\n",
    "    print(\"Train accuracy:\", calculate_accuracy(X_train, Y_train, model))\n",
    "    print(\"Validation accuracy:\", calculate_accuracy(X_val, Y_val, model))\n",
    "\n",
    "    # Plot loss for first model (task 2b)\n",
    "    plt.ylim([0., .2])\n",
    "    utils.plot_loss(train_history[\"loss\"],\n",
    "                    \"Training Loss\", npoints_to_average=10)\n",
    "    utils.plot_loss(val_history[\"loss\"], \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Cross Entropy Loss - Average\")\n",
    "    plt.savefig(\"task2b_binary_train_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.ylim([0.93, .99])\n",
    "    utils.plot_loss(train_history[\"accuracy\"], \"Training Accuracy\")\n",
    "    utils.plot_loss(val_history[\"accuracy\"], \"Validation Accuracy\")\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"task2b_binary_train_accuracy.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py\n",
    "from pickle import NONE\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "class BaseTrainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            learning_rate: float,\n",
    "            batch_size: int,\n",
    "            shuffle_dataset: bool,\n",
    "            X_train: np.ndarray, Y_train: np.ndarray,\n",
    "            X_val: np.ndarray, Y_val: np.ndarray,) -> None:\n",
    "        \"\"\"\n",
    "            Initialize the trainer responsible for performing the gradient descent loop.\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.shuffle_dataset = shuffle_dataset\n",
    "\n",
    "    def validation_step(self):\n",
    "        \"\"\"\n",
    "        Perform a validation step to evaluate the model at the current step for the validation set.\n",
    "        Also calculates the current accuracy of the model on the train set.\n",
    "        Returns:\n",
    "            loss (float): cross entropy loss over the whole dataset\n",
    "            accuracy_ (float): accuracy over the whole dataset\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "            accuracy_train (float): Accuracy on train dataset\n",
    "            accuracy_val (float): Accuracy on the validation dataset\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "            Perform forward, backward and gradient descent step here.\n",
    "        Args:\n",
    "            X: one batch of images\n",
    "            Y: one batch of labels\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            num_epochs: int):\n",
    "        \"\"\"\n",
    "        Training loop for model.\n",
    "        Implements stochastic gradient descent with num_epochs passes over the train dataset.\n",
    "        Returns:\n",
    "            train_history: a dictionary containing loss and accuracy over all training steps\n",
    "            val_history: a dictionary containing loss and accuracy over a selected set of steps\n",
    "        \"\"\"\n",
    "        # Utility variables\n",
    "        num_batches_per_epoch = self.X_train.shape[0] // self.batch_size\n",
    "        num_steps_per_val = num_batches_per_epoch // 5\n",
    "        # A tracking value of loss over all training steps\n",
    "        train_history = dict(\n",
    "            loss={},\n",
    "            accuracy={}\n",
    "        )\n",
    "        val_history = dict(\n",
    "            loss={},\n",
    "            accuracy={}\n",
    "        )\n",
    "\n",
    "        global_step = 0\n",
    "        best = None\n",
    "        counting = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loader = utils.batch_loader(\n",
    "                self.X_train, self.Y_train, self.batch_size, shuffle=self.shuffle_dataset)\n",
    "            for X_batch, Y_batch in iter(train_loader):\n",
    "                loss = self.train_step(X_batch, Y_batch)\n",
    "                # Track training loss continuously\n",
    "                train_history[\"loss\"][global_step] = loss\n",
    "\n",
    "                # Track validation loss / accuracy every time we progress 20% through the dataset\n",
    "                if global_step % num_steps_per_val == 0:\n",
    "                    val_loss, accuracy_train, accuracy_val = self.validation_step()\n",
    "                    train_history[\"accuracy\"][global_step] = accuracy_train\n",
    "                    val_history[\"loss\"][global_step] = val_loss\n",
    "                    val_history[\"accuracy\"][global_step] = accuracy_val\n",
    "\n",
    "                    # TODO (Task 2d): Implement early stopping here.\n",
    "                    # You can access the validation loss in val_history[\"loss\"]\n",
    "                    if best == None:\n",
    "                        best = val_loss\n",
    "                    elif val_loss < best:\n",
    "                        counting = 0\n",
    "                        best = val_loss\n",
    "                    else:\n",
    "                        counting += 1\n",
    "                if counting == 10:\n",
    "                    print(\"global step1: \", global_step)\n",
    "                    print(\"Training stopped on epoch\", epoch)\n",
    "                    return train_history, val_history\n",
    "                \n",
    "\n",
    "                global_step += 1            \n",
    "        return train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from task2a import cross_entropy_loss, BinaryModel, pre_process_images\n",
    "from trainer import BaseTrainer\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def calculate_accuracy(X: np.ndarray, targets: np.ndarray, model: BinaryModel) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 785]\n",
    "        targets: labels/targets of each image of shape: [batch size, 1]\n",
    "        model: model of class BinaryModel\n",
    "    Returns:\n",
    "        Accuracy (float)\n",
    "    \"\"\"\n",
    "    # TODO Implement this function (Task 2c)\n",
    "    accuracy = 0.0\n",
    "    correct_predictions = 0\n",
    "    predictions = targets.shape[0]\n",
    "    outputs = model.forward(X)\n",
    "    \n",
    "    for idx, val in enumerate(outputs):\n",
    "        target = targets[idx]\n",
    "        if val >= 0.5 and target == 1 or val<0.5 and target == 0:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions/predictions\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class LogisticTrainer(BaseTrainer):\n",
    "    def train_step(self, X_batch: np.ndarray, Y_batch: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform forward, backward and gradient descent step here.\n",
    "        The function is called once for every batch (see trainer.py) to perform the train step.\n",
    "        The function returns the mean loss value which is then automatically logged in our variable self.train_history.\n",
    "\n",
    "        Args:\n",
    "            X: one batch of images\n",
    "            Y: one batch of labels\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function (task 2b)\n",
    "        outputs = model.forward(X_batch)                \n",
    "        loss = cross_entropy_loss(Y_batch, outputs)\n",
    "        model.backward(X_batch,outputs,Y_batch)\n",
    "        model.w = model.w-learning_rate*model.grad\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self):\n",
    "        \"\"\"\n",
    "        Perform a validation step to evaluate the model at the current step for the validation set.\n",
    "        Also calculates the current accuracy of the model on the train set.\n",
    "        Returns:\n",
    "            loss (float): cross entropy loss over the whole dataset\n",
    "            accuracy_ (float): accuracy over the whole dataset\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "            accuracy_train (float): Accuracy on train dataset\n",
    "            accuracy_val (float): Accuracy on the validation dataset\n",
    "        \"\"\"\n",
    "        # NO NEED TO CHANGE THIS FUNCTION\n",
    "        logits = self.model.forward(self.X_val)\n",
    "        loss = cross_entropy_loss(Y_val, logits)\n",
    "\n",
    "        accuracy_train = calculate_accuracy(\n",
    "            X_train, Y_train, self.model)\n",
    "        accuracy_val = calculate_accuracy(\n",
    "            X_val, Y_val, self.model)\n",
    "        return loss, accuracy_train, accuracy_val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # hyperparameters DO NOT CHANGE IF NOT SPECIFIED IN ASSIGNMENT TEXT\n",
    "    num_epochs = 500\n",
    "    learning_rate = 0.05\n",
    "    batch_size = 128\n",
    "    shuffle_dataset = False\n",
    "\n",
    "    # Load dataset\n",
    "    category1, category2 = 2, 3\n",
    "    X_train, Y_train, X_val, Y_val = utils.load_binary_dataset(\n",
    "        category1, category2)\n",
    "\n",
    "    X_train = pre_process_images(X_train)\n",
    "    X_val = pre_process_images(X_val)\n",
    "\n",
    "    # ANY PARTS OF THE CODE BELOW THIS CAN BE CHANGED.\n",
    "\n",
    "    # Intialize model\n",
    "    model = BinaryModel()\n",
    "    # Train model\n",
    "    trainer = LogisticTrainer(\n",
    "        model, learning_rate, batch_size, shuffle_dataset,\n",
    "        X_train, Y_train, X_val, Y_val,\n",
    "    )\n",
    "    train_history, val_history = trainer.train(num_epochs)\n",
    "\n",
    "    # Plot and print everything you want of information\n",
    "\n",
    "    print(\"Final Train Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_train, model.forward(X_train)))\n",
    "    print(\"Final Validation Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_val, model.forward(X_val)))\n",
    "    print(\"Train accuracy:\", calculate_accuracy(X_train, Y_train, model))\n",
    "    print(\"Validation accuracy:\", calculate_accuracy(X_val, Y_val, model))\n",
    "\n",
    "    # Plot loss for first model (task 2b)\n",
    "    plt.ylim([0., .2])\n",
    "    utils.plot_loss(train_history[\"loss\"],\n",
    "                    \"Training Loss\", npoints_to_average=10)\n",
    "    utils.plot_loss(val_history[\"loss\"], \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Cross Entropy Loss - Average\")\n",
    "    plt.savefig(\"task2b_binary_train_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.ylim([0.93, .99])\n",
    "    utils.plot_loss(train_history[\"accuracy\"], \"Training Accuracy\")\n",
    "    utils.plot_loss(val_history[\"accuracy\"], \"Validation Accuracy\")\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"task2b_binary_train_accuracy.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task2.py\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from task2a import cross_entropy_loss, BinaryModel, pre_process_images\n",
    "from trainer import BaseTrainer\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def calculate_accuracy(X: np.ndarray, targets: np.ndarray, model: BinaryModel) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 785]\n",
    "        targets: labels/targets of each image of shape: [batch size, 1]\n",
    "        model: model of class BinaryModel\n",
    "    Returns:\n",
    "        Accuracy (float)\n",
    "    \"\"\"\n",
    "    # TODO Implement this function (Task 2c)\n",
    "    accuracy = 0.0\n",
    "    correct_predictions = 0\n",
    "    predictions = targets.shape[0]\n",
    "    outputs = model.forward(X)\n",
    "    \n",
    "    #print(\"shape target: \",targets.shape,\"   shape out: \", outputs.shape)\n",
    "    for idx, val in enumerate(outputs):\n",
    "        target = targets[idx]\n",
    "        if val >= 0.5 and target == 1 or val<0.5 and target == 0:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions/predictions\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class LogisticTrainer(BaseTrainer):\n",
    "\n",
    "    def train_step(self, X_batch: np.ndarray, Y_batch: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform forward, backward and gradient descent step here.\n",
    "        The function is called once for every batch (see trainer.py) to perform the train step.\n",
    "        The function returns the mean loss value which is then automatically logged in our variable self.train_history.\n",
    "\n",
    "        Args:\n",
    "            X: one batch of images\n",
    "            Y: one batch of labels\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function (task 2b)\n",
    "        outputs = model.forward(X_batch)                \n",
    "        loss = cross_entropy_loss(Y_batch, outputs)\n",
    "        model.backward(X_batch,outputs,Y_batch)\n",
    "        model.w = model.w-learning_rate*model.grad\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self):\n",
    "        \"\"\"\n",
    "        Perform a validation step to evaluate the model at the current step for the validation set.\n",
    "        Also calculates the current accuracy of the model on the train set.\n",
    "        Returns:\n",
    "            loss (float): cross entropy loss over the whole dataset\n",
    "            accuracy_ (float): accuracy over the whole dataset\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "            accuracy_train (float): Accuracy on train dataset\n",
    "            accuracy_val (float): Accuracy on the validation dataset\n",
    "        \"\"\"\n",
    "        # NO NEED TO CHANGE THIS FUNCTION\n",
    "        logits = self.model.forward(self.X_val)\n",
    "        loss = cross_entropy_loss(Y_val, logits)\n",
    "\n",
    "        accuracy_train = calculate_accuracy(\n",
    "            X_train, Y_train, self.model)\n",
    "        accuracy_val = calculate_accuracy(\n",
    "            X_val, Y_val, self.model)\n",
    "        return loss, accuracy_train, accuracy_val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # hyperparameters DO NOT CHANGE IF NOT SPECIFIED IN ASSIGNMENT TEXT\n",
    "    num_epochs = 500\n",
    "    learning_rate = 0.05\n",
    "    batch_size = 128\n",
    "    shuffle_dataset = False\n",
    "\n",
    "    # Load dataset\n",
    "    category1, category2 = 2, 3\n",
    "    X_train, Y_train, X_val, Y_val = utils.load_binary_dataset(\n",
    "        category1, category2)\n",
    "\n",
    "    X_train = pre_process_images(X_train)\n",
    "    X_val = pre_process_images(X_val)\n",
    "\n",
    "    # ANY PARTS OF THE CODE BELOW THIS CAN BE CHANGED.\n",
    "\n",
    "    # Intialize model\n",
    "    model = BinaryModel()\n",
    "    # Train model\n",
    "    trainer = LogisticTrainer(\n",
    "        model, learning_rate, batch_size, shuffle_dataset,\n",
    "        X_train, Y_train, X_val, Y_val,\n",
    "    )\n",
    "    train_history, val_history = trainer.train(num_epochs)\n",
    "\n",
    "    # Plot and print everything you want of information\n",
    "\n",
    "    print(\"Final Train Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_train, model.forward(X_train)))\n",
    "    print(\"Final Validation Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_val, model.forward(X_val)))\n",
    "    print(\"Train accuracy:\", calculate_accuracy(X_train, Y_train, model))\n",
    "    print(\"Validation accuracy:\", calculate_accuracy(X_val, Y_val, model))\n",
    "\n",
    "    # Plot loss for first model (task 2b)\n",
    "    plt.ylim([0., .2])\n",
    "    utils.plot_loss(train_history[\"loss\"],\n",
    "                    \"Training Loss\", npoints_to_average=10)\n",
    "    utils.plot_loss(val_history[\"loss\"], \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Cross Entropy Loss - Average\")\n",
    "    plt.savefig(\"task2b_binary_train_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.ylim([0.93, .99])\n",
    "    utils.plot_loss(train_history[\"accuracy\"], \"Training Accuracy\")\n",
    "    utils.plot_loss(val_history[\"accuracy\"], \"Validation Accuracy\")\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"task2b_binary_train_accuracy.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Task 2e - Create a comparison between training with and without shuffling\n",
    "    shuffle_dataset = True\n",
    "    # Intialize model\n",
    "    model = BinaryModel()\n",
    "    # Train model\n",
    "    trainer = LogisticTrainer(\n",
    "        model, learning_rate, batch_size, shuffle_dataset,\n",
    "        X_train, Y_train, X_val, Y_val,\n",
    "    )\n",
    "    train_history_shuffle, val_history_shuffle = trainer.train(num_epochs)\n",
    "\n",
    "    plt.ylim([0., .2])\n",
    "    utils.plot_loss(train_history[\"loss\"],\n",
    "                    \"Training Loss\", npoints_to_average=10)\n",
    "    utils.plot_loss(\n",
    "        train_history_shuffle[\"loss\"], \"Training Loss with shuffle\", npoints_to_average=10)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Cross Entropy Loss - Average\")\n",
    "    plt.savefig(\"task2e_train_loss_with_shuffle.png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.ylim([0.93, .99])\n",
    "    utils.plot_loss(val_history[\"accuracy\"], \"Validation Accuracy\")\n",
    "    utils.plot_loss(\n",
    "        val_history_shuffle[\"accuracy\"], \"Validation Accuracy with shuffle\")\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"task2e_train_accuracy_shuffle_difference.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task3a.py\n",
    "import numpy as np\n",
    "import utils\n",
    "from task2a import pre_process_images\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(targets: np.ndarray, outputs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        targets: labels/targets of each image of shape: [batch size, num_classes]\n",
    "        outputs: outputs of model of shape: [batch size, num_classes]\n",
    "    Returns:\n",
    "        Cross entropy error (float)\n",
    "    \"\"\"\n",
    "    # TODO implement this function (Task 3a)\n",
    "    assert targets.shape == outputs.shape,\\\n",
    "        f\"Targets shape: {targets.shape}, outputs: {outputs.shape}\"\n",
    "    N = targets.shape[0]\n",
    "    C_n = -targets*(np.log(outputs))\n",
    "    loss = np.sum(C_n)/N\n",
    "    return loss\n",
    "\n",
    "\n",
    "class SoftmaxModel:\n",
    "    def __init__(self, l2_reg_lambda: float):\n",
    "        # Define number of input nodes\n",
    "        self.I = 785\n",
    "\n",
    "        # Define number of output nodes\n",
    "        self.num_outputs = 10\n",
    "        self.w = np.zeros((self.I, self.num_outputs))\n",
    "        self.grad = None\n",
    "\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "        Returns:\n",
    "            y: output of model with shape [batch size, num_outputs]\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO implement this function (Task 3a)\n",
    "        z = X.dot(self.w)\n",
    "        y = np.exp(z)/np.sum(np.exp(z), axis = 1, keepdims=True) #alternativly to keepdims transpose both num and den\n",
    "        return y\n",
    "        # return None\n",
    "\n",
    "    def backward(self, X: np.ndarray, outputs: np.ndarray, targets: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Computes the gradient and saves it to the variable self.grad\n",
    "\n",
    "        Args:\n",
    "            X: images of shape [batch size, 785]\n",
    "            outputs: outputs of model of shape: [batch size, num_outputs]\n",
    "            targets: labels/targets of each image of shape: [batch size, num_classes]\n",
    "        \"\"\"\n",
    "        # TODO implement this function (Task 3a)\n",
    "        # To implement L2 regularization task (4b) you can get the lambda value in self.l2_reg_lambda \n",
    "        # which is defined in the constructor.\n",
    "        assert targets.shape == outputs.shape,\\\n",
    "            f\"Output shape: {outputs.shape}, targets: {targets.shape}\"\n",
    "        self.grad = np.zeros_like(self.w)\n",
    "        assert self.grad.shape == self.w.shape,\\\n",
    "             f\"Grad shape: {self.grad.shape}, w: {self.w.shape}\"\n",
    "        self.grad = -(targets - outputs).T.dot(X)/X.shape[0]\n",
    "        self.grad = self.grad.T\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "def one_hot_encode(Y: np.ndarray, num_classes: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Y: shape [Num examples, 1]\n",
    "        num_classes: Number of classes to use for one-hot encoding\n",
    "    Returns:\n",
    "        Y: shape [Num examples, num classes]\n",
    "    \"\"\"\n",
    "    # TODO implement this function (Task 3a)\n",
    "    Y = np.eye(num_classes)[Y]\n",
    "    Y = np.squeeze(Y, axis=1)\n",
    "    return Y\n",
    "\n",
    "    # raise NotImplementedError\n",
    "\n",
    "\n",
    "def gradient_approximation_test(model: SoftmaxModel, X: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "        Numerical approximation for gradients. Should not be edited. \n",
    "        Details about this test is given in the appendix in the assignment.\n",
    "    \"\"\"\n",
    "    w_orig = np.random.normal(loc=0, scale=1/model.w.shape[0]**2, size=model.w.shape)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    for i in range(model.w.shape[0]):\n",
    "        for j in range(model.w.shape[1]):\n",
    "            model.w = w_orig.copy()\n",
    "            orig = model.w[i, j].copy()\n",
    "            model.w[i, j] = orig + epsilon\n",
    "            logits = model.forward(X)\n",
    "            cost1 = cross_entropy_loss(Y, logits)\n",
    "            model.w[i, j] = orig - epsilon\n",
    "            logits = model.forward(X)\n",
    "            cost2 = cross_entropy_loss(Y, logits)\n",
    "            gradient_approximation = (cost1 - cost2) / (2 * epsilon)\n",
    "            model.w[i, j] = orig\n",
    "            # Actual gradient\n",
    "            logits = model.forward(X)\n",
    "            model.backward(X, logits, Y)\n",
    "            difference = gradient_approximation - model.grad[i, j]\n",
    "            assert abs(difference) <= epsilon**2,\\\n",
    "                f\"Calculated gradient is incorrect. \" \\\n",
    "                f\"Approximation: {gradient_approximation}, actual gradient: {model.grad[i, j]}\\n\" \\\n",
    "                f\"If this test fails there could be errors in your cross entropy loss function, \" \\\n",
    "                f\"forward function or backward function\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Simple test on one-hot encoding\n",
    "    Y = np.zeros((1, 1), dtype=int)\n",
    "    Y[0, 0] = 3\n",
    "    Y = one_hot_encode(Y, 10)\n",
    "    assert Y[0, 3] == 1 and Y.sum() == 1, \\\n",
    "        f\"Expected the vector to be [0,0,0,1,0,0,0,0,0,0], but got {Y}\"\n",
    "\n",
    "    X_train, Y_train, *_ = utils.load_full_mnist()\n",
    "    X_train = pre_process_images(X_train)\n",
    "    Y_train = one_hot_encode(Y_train, 10)\n",
    "    assert X_train.shape[1] == 785,\\\n",
    "        f\"Expected X_train to have 785 elements per image. Shape was: {X_train.shape}\"\n",
    "\n",
    "    # Simple test for forward pass. Note that this does not cover all errors!\n",
    "    model = SoftmaxModel(0.0)\n",
    "    logits = model.forward(X_train)\n",
    "    np.testing.assert_almost_equal(\n",
    "        logits.mean(), 1/10,\n",
    "        err_msg=\"Since the weights are all 0's, the softmax activation should be 1/10\")\n",
    "\n",
    "    # Gradient approximation check for 100 images\n",
    "    X_train = X_train[:100]\n",
    "    Y_train = Y_train[:100]\n",
    "    for i in range(2):\n",
    "        gradient_approximation_test(model, X_train, Y_train)\n",
    "        model.w = np.random.randn(*model.w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task3.py\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from task2a import pre_process_images\n",
    "from trainer import BaseTrainer\n",
    "from task3a import cross_entropy_loss, SoftmaxModel, one_hot_encode\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def calculate_accuracy(X: np.ndarray, targets: np.ndarray, model: SoftmaxModel) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 785]\n",
    "        targets: labels/targets of each image of shape: [batch size, 10]\n",
    "        model: model of class SoftmaxModel\n",
    "    Returns:\n",
    "        Accuracy (float)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function (task 3c)\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class SoftmaxTrainer(BaseTrainer):\n",
    "\n",
    "    def train_step(self, X_batch: np.ndarray, Y_batch: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform forward, backward and gradient descent step here.\n",
    "        The function is called once for every batch (see trainer.py) to perform the train step.\n",
    "        The function returns the mean loss value which is then automatically logged in our variable self.train_history.\n",
    "\n",
    "        Args:\n",
    "            X: one batch of images\n",
    "            Y: one batch of labels\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function (task 3b)\n",
    "        outputs = model.forward(X_batch)                \n",
    "        loss = cross_entropy_loss(Y_batch, outputs)\n",
    "        model.backward(X_batch,outputs,Y_batch)\n",
    "        model.w = model.w-learning_rate*model.grad\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self):\n",
    "        \"\"\"\n",
    "        Perform a validation step to evaluate the model at the current step for the validation set.\n",
    "        Also calculates the current accuracy of the model on the train set.\n",
    "        Returns:\n",
    "            loss (float): cross entropy loss over the whole dataset\n",
    "            accuracy_ (float): accuracy over the whole dataset\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "            accuracy_train (float): Accuracy on train dataset\n",
    "            accuracy_val (float): Accuracy on the validation dataset\n",
    "        \"\"\"\n",
    "        # NO NEED TO CHANGE THIS FUNCTION\n",
    "        logits = self.model.forward(self.X_val)\n",
    "        loss = cross_entropy_loss(Y_val, logits)\n",
    "\n",
    "        accuracy_train = calculate_accuracy(\n",
    "            X_train, Y_train, self.model)\n",
    "        accuracy_val = calculate_accuracy(\n",
    "            X_val, Y_val, self.model)\n",
    "        return loss, accuracy_train, accuracy_val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # hyperparameters DO NOT CHANGE IF NOT SPECIFIED IN ASSIGNMENT TEXT\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.01\n",
    "    batch_size = 128\n",
    "    l2_reg_lambda = 0\n",
    "    shuffle_dataset = True\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, Y_train, X_val, Y_val = utils.load_full_mnist()\n",
    "    X_train = pre_process_images(X_train)\n",
    "    X_val = pre_process_images(X_val)\n",
    "    Y_train = one_hot_encode(Y_train, 10)\n",
    "    Y_val = one_hot_encode(Y_val, 10)\n",
    "\n",
    "    # ANY PARTS OF THE CODE BELOW THIS CAN BE CHANGED.\n",
    "\n",
    "    # Intialize model\n",
    "    model = SoftmaxModel(l2_reg_lambda)\n",
    "    # Train model\n",
    "    trainer = SoftmaxTrainer(\n",
    "        model, learning_rate, batch_size, shuffle_dataset,\n",
    "        X_train, Y_train, X_val, Y_val,\n",
    "    )\n",
    "    train_history, val_history = trainer.train(num_epochs)\n",
    "\n",
    "    print(\"Final Train Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_train, model.forward(X_train)))\n",
    "    print(\"Final Validation Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_val, model.forward(X_val)))\n",
    "    print(\"Final Train accuracy:\", calculate_accuracy(X_train, Y_train, model))\n",
    "    print(\"Final Validation accuracy:\", calculate_accuracy(X_val, Y_val, model))\n",
    "\n",
    "    plt.ylim([0.2, .6])\n",
    "    utils.plot_loss(train_history[\"loss\"],\n",
    "                    \"Training Loss\", npoints_to_average=10)\n",
    "    utils.plot_loss(val_history[\"loss\"], \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Cross Entropy Loss - Average\")\n",
    "    plt.savefig(\"task3b_softmax_train_loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# task3.py\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from task2a import pre_process_images\n",
    "from trainer import BaseTrainer\n",
    "from task3a import cross_entropy_loss, SoftmaxModel, one_hot_encode\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def calculate_accuracy(X: np.ndarray, targets: np.ndarray, model: SoftmaxModel) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: images of shape [batch size, 785]\n",
    "        targets: labels/targets of each image of shape: [batch size, 10]\n",
    "        model: model of class SoftmaxModel\n",
    "    Returns:\n",
    "        Accuracy (float)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function (task 3c)\n",
    "    accuracy = 0.0\n",
    "    correct_predictions = 0\n",
    "    predictions = targets.shape[0]\n",
    "    outputs = model.forward(X)\n",
    "    \n",
    "    #print(\"shape target: \",targets.shape,\"   shape out: \", outputs.shape)\n",
    "    for idx, val in enumerate(outputs):\n",
    "        target = targets[idx]\n",
    "        print(f\"Target: {target}\")\n",
    "        print(f\"Value: {val}\")\n",
    "        print(f\"Outputs: {outputs}\")\n",
    "        for i in range(target.shape[0]):    \n",
    "            if val[i] >= 0.5 and target[i] == 1 or val[i] < 0.5 and target[i] == 0:\n",
    "                correct_predictions += 1   \n",
    "    accuracy = correct_predictions/predictions\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class SoftmaxTrainer(BaseTrainer):\n",
    "\n",
    "    def train_step(self, X_batch: np.ndarray, Y_batch: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform forward, backward and gradient descent step here.\n",
    "        The function is called once for every batch (see trainer.py) to perform the train step.\n",
    "        The function returns the mean loss value which is then automatically logged in our variable self.train_history.\n",
    "\n",
    "        Args:\n",
    "            X: one batch of images\n",
    "            Y: one batch of labels\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function (task 3b)\n",
    "        outputs = model.forward(X_batch)                \n",
    "        loss = cross_entropy_loss(Y_batch, outputs)\n",
    "        model.backward(X_batch,outputs,Y_batch)\n",
    "        model.w = model.w-learning_rate*model.grad\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self):\n",
    "        \"\"\"\n",
    "        Perform a validation step to evaluate the model at the current step for the validation set.\n",
    "        Also calculates the current accuracy of the model on the train set.\n",
    "        Returns:\n",
    "            loss (float): cross entropy loss over the whole dataset\n",
    "            accuracy_ (float): accuracy over the whole dataset\n",
    "        Returns:\n",
    "            loss value (float) on batch\n",
    "            accuracy_train (float): Accuracy on train dataset\n",
    "            accuracy_val (float): Accuracy on the validation dataset\n",
    "        \"\"\"\n",
    "        # NO NEED TO CHANGE THIS FUNCTION\n",
    "        logits = self.model.forward(self.X_val)\n",
    "        loss = cross_entropy_loss(Y_val, logits)\n",
    "\n",
    "        accuracy_train = calculate_accuracy(\n",
    "            X_train, Y_train, self.model)\n",
    "        accuracy_val = calculate_accuracy(\n",
    "            X_val, Y_val, self.model)\n",
    "        return loss, accuracy_train, accuracy_val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # hyperparameters DO NOT CHANGE IF NOT SPECIFIED IN ASSIGNMENT TEXT\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.01\n",
    "    batch_size = 128\n",
    "    l2_reg_lambda = 0\n",
    "    shuffle_dataset = True\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, Y_train, X_val, Y_val = utils.load_full_mnist()\n",
    "    X_train = pre_process_images(X_train)\n",
    "    X_val = pre_process_images(X_val)\n",
    "    Y_train = one_hot_encode(Y_train, 10)\n",
    "    Y_val = one_hot_encode(Y_val, 10)\n",
    "\n",
    "    # ANY PARTS OF THE CODE BELOW THIS CAN BE CHANGED.\n",
    "\n",
    "    # Intialize model\n",
    "    model = SoftmaxModel(l2_reg_lambda)\n",
    "    # Train model\n",
    "    trainer = SoftmaxTrainer(\n",
    "        model, learning_rate, batch_size, shuffle_dataset,\n",
    "        X_train, Y_train, X_val, Y_val,\n",
    "    )\n",
    "    train_history, val_history = trainer.train(num_epochs)\n",
    "\n",
    "    print(\"Final Train Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_train, model.forward(X_train)))\n",
    "    print(\"Final Validation Cross Entropy Loss:\",\n",
    "          cross_entropy_loss(Y_val, model.forward(X_val)))\n",
    "    print(\"Final Train accuracy:\", calculate_accuracy(X_train, Y_train, model))\n",
    "    print(\"Final Validation accuracy:\", calculate_accuracy(X_val, Y_val, model))\n",
    "\n",
    "    plt.ylim([0.2, .6])\n",
    "    utils.plot_loss(train_history[\"loss\"],\n",
    "                    \"Training Loss\", npoints_to_average=10)\n",
    "    utils.plot_loss(val_history[\"loss\"], \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Cross Entropy Loss - Average\")\n",
    "    plt.savefig(\"task3b_softmax_train_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.ylim([0.89, .93])\n",
    "    utils.plot_loss(train_history[\"accuracy\"], \"Training Accuracy\")\n",
    "    utils.plot_loss(val_history[\"accuracy\"], \"Validation Accuracy\")\n",
    "    plt.xlabel(\"Number of Training Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"task3b_softmax_train_accuracy.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "\n",
    "Something about overfitting or something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Fill in image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
