{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c87f46",
   "metadata": {},
   "source": [
    "# Assignment 3 Group 116\n",
    "\n",
    "##### By Jonas Halle and Alexander Rambech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c11172",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967454a6",
   "metadata": {},
   "source": [
    "### Task 1a)\n",
    "\n",
    "We choose to do the convolutions using 1 layer of zero padding around the edge of the original image and utilizing a stride length of $S = 1$. This results in the $3 \\times 5$ image:\n",
    "\n",
    "$ \\begin{bmatrix}\n",
    "2 & -1 & 11 & -2 & -13 \\\\\n",
    "10 & -4 & 8 & 2 & -18 \\\\\n",
    "14 & -1 & -5 & 6 & 9 \n",
    "\\end{bmatrix}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede190c",
   "metadata": {},
   "source": [
    "### Task 1b)\n",
    "\n",
    "Translational variations are hard to pick up on by normal neural nets, because they don't have the spatial link needed to recognize that the subject of the picture can be in different locations in the frame. CNNs use can use kernel convolutions to check for different features in an image and thereby finding the subject in the image regardless of position in the frame. These features are found in the convolutional layers of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204f958",
   "metadata": {},
   "source": [
    "### Task 1c)\n",
    "\n",
    "You would need $2$ layers of padding so that the center pixel of the kernel reaches the outer edges of the input image. Meaning $P = 2$. This can also be checked using the equation \n",
    "\n",
    "$W_2 = \\frac{W_1 - F + 2P}{S}+1$, \n",
    "\n",
    "where $W_2$ is the width of the output layer, $W_1$ is the width of the input layer, $F$ is the size of the square kernel, $S$ is the stride length and $P$ is the padding. Since the goal is that the ouput layer has the same width as the input layer, meaning $W_2 = W_1$ and $S = 1$, the equation for $P$ can be written as:\n",
    "\n",
    "$P = F - 1 = 3 - 1 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0661a2",
   "metadata": {},
   "source": [
    "### Task 1d)\n",
    "\n",
    "We have that $W_2 = \\frac{W_1 - F + 2P}{s} + 1 \\implies F = W_1 + 2P - \\frac{W_2}{s} + 1 = 512 + 2 \\cdot 0 - \\frac{504}{1} = 9$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d953d",
   "metadata": {},
   "source": [
    "### Task 1e)\n",
    "\n",
    "$W_2 = \\frac{W_1 - F + 2P}{S} + 1 = \\frac{504 - 2 + 2 \\cdot 0}{2} + 1 = 252$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360071c",
   "metadata": {},
   "source": [
    "### Task 1f)\n",
    "\n",
    "$W_2 = \\frac{252 - 3 + 2 \\cdot 0}{1} + 1 = 250$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e0621",
   "metadata": {},
   "source": [
    "### Task 1g)\n",
    "\n",
    "Since the input is a square RGB image of size $32$, we have $3$ channels and thereby an input of size $32 \\times 32 \\times 3$. This means that the $5 \\times 5$ convolutional moving with a stride of $S = 1$ with padding $P = 2$ makes a new layer of the exact same size. \n",
    "\n",
    "The number of parameters in the first convolutional layer is given by the size of the kernel, which are the weights, the number of channels pluss the bias. Times this with the number of filters and we have the number of parameters. The number of parameters in the first layer is thereby given by:\n",
    "\n",
    "$((5 \\cdot 5 \\cdot 3) + 1 ) \\cdot 32 = 2,432$\n",
    "\n",
    "Since the Max-Pooling layer only shrinks the dimension of a channel itself, but not the number of feature maps, the pooling layer has no effect on the amound of parameters in the network. \n",
    "\n",
    "To find the number of parameters for the next layers, we can use the same $((F \\cdot F \\cdot D_1) + b) \\cdot D_2 = N$ formula as above, where $F$ is the size of the kernel, $b$ is the number of biases, $D_1$ is the number of features in the previous layer and $D_2$ is the number of features in the next layer. For the two next convolutional layers this is:\n",
    "\n",
    "$((5 \\cdot 5 \\cdot 32) + 1) \\cdot 64 = 51,264$\n",
    "\n",
    "$((5 \\cdot 5 \\cdot 64) + 1) \\cdot 128 = 204,928$\n",
    "\n",
    "The width of the input channels is $W_i = 32$ and is halfed with each pooling layer, when it's time to flatten after the last pooling, the dimensions of each feature map is $4 \\times 4$. This yields:\n",
    "\n",
    "$((4 \\cdot 4 \\cdot 128) + 1) \\cdot 64 = 131,136$\n",
    "\n",
    "The number of parameters in the last layer is simply given by:\n",
    "\n",
    "$(64 + 1) \\cdot 10 = 650\n",
    "\n",
    "This gives a total parameters of: \n",
    "\n",
    "$2,432 + 51,264 + 204928 + 131,136 + 650 = 390,410$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a176581",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaee0bd",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "\n",
    "Early stopping kicks in before epoch 10 and we reach a validation and test accuracy north of 70%, which we regard as pretty good given that we have not started tuning or messing with different techniques for making the CNN better.\n",
    "\n",
    "![](2a_fig_with_validation_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af63ce4",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "\n",
    "Final validation and test accuracies are 73.4% and 72.9%, respectivley.\n",
    "\n",
    "![](2b_snipp.png)\n",
    "\n",
    "![](2b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c1324",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839ef48",
   "metadata": {},
   "source": [
    "### Task 3a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab33e0",
   "metadata": {},
   "source": [
    "#### Model 1\n",
    "\n",
    "For the first model we have gone for a very simple architecture with only 2 convolutional layers in the feature extractor and the same classifier as we used in the previous task, but with added batch normalization. \n",
    "\n",
    "Batch normalization in the after the convolutional layers had some effect, but this technique was very effective in the classifier, yielding several persentage points in test accuracy. \n",
    "\n",
    "We tune learning rate and momentum to get the best result possible. Learning rate = 10e-2 seems to be quite optimal and momentum = 0.5 gives as the best result. \n",
    "\n",
    "L2 regularization in SGD only made things worse! \n",
    "\n",
    "MaxPooling with 4x4 and stride 4, makes the network slightly worse. \n",
    "\n",
    "We tried removing the MaxPool after the first Conv layer and replace it with strides of two in the conv layer. This resulted in worse test accuracy.\n",
    "\n",
    "Removing all pooling resulted in even worse results than replacing just the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3b0a4",
   "metadata": {},
   "source": [
    "#### Model 2\n",
    "\n",
    "It's all about the layers\n",
    "\n",
    "The first version of the second model consists of the same classifier as in Task 2 and a feature extractor with 6 Conv layers and MaxPooling between every other layer. This network managed to reach a best of 76.9% test accuracy, without any batch normalization in any layer and using the same optimizer and learning rate that was optimal for Model 1. \n",
    "\n",
    "We experiensed again that doing batch normalization in the classifier was very effective!\n",
    "\n",
    "When changing optimizer to Adam, we got even better results with a best test accuracy og 83.7%\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
