{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c87f46",
   "metadata": {},
   "source": [
    "# Assignment 3 Group 116\n",
    "\n",
    "##### By Jonas Halle and Alexander Rambech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c11172",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967454a6",
   "metadata": {},
   "source": [
    "### Task 1a)\n",
    "\n",
    "We choose to do the convolutions using 1 layer of zero padding around the edge of the original image and utilizing a stride length of $S = 1$. This results in the $3 \\times 5$ image:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "2 & -1 & 11 & -2 & -13 \\\\\n",
    "10 & -4 & 8 & 2 & -18 \\\\\n",
    "14 & -1 & -5 & 6 & 9 \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede190c",
   "metadata": {},
   "source": [
    "### Task 1b)\n",
    "\n",
    "Translational variations are hard to pick up on by normal neural nets, because they don't have the spatial link needed to recognize that the subject of the picture can be in different locations in the frame. CNNs use can use kernel convolutions to check for different features in an image and thereby finding the subject in the image regardless of position in the frame. These features are found in the convolutional layers of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204f958",
   "metadata": {},
   "source": [
    "### Task 1c)\n",
    "\n",
    "You would need $2$ layers of padding so that the center pixel of the kernel reaches the outer edges of the input image. Meaning $P = 2$. This can also be checked using the equation \n",
    "\n",
    "$W_2 = \\frac{W_1 - F + 2P}{S}+1$, \n",
    "\n",
    "where $W_2$ is the width of the output layer, $W_1$ is the width of the input layer, $F$ is the size of the square kernel, $S$ is the stride length and $P$ is the padding. Since the goal is that the ouput layer has the same width as the input layer, meaning $W_2 = W_1$ and $S = 1$, the equation for $P$ can be written as:\n",
    "\n",
    "$P = F - 1 = 3 - 1 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0661a2",
   "metadata": {},
   "source": [
    "### Task 1d)\n",
    "\n",
    "We have that $W_2 = \\frac{W_1 - F + 2P}{s} + 1 \\implies F = W_1 + 2P - \\frac{W_2}{s} + 1 = 512 + 2 \\cdot 0 - \\frac{504}{1} = 9$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d953d",
   "metadata": {},
   "source": [
    "### Task 1e)\n",
    "\n",
    "$W_2 = \\frac{W_1 - F + 2P}{S} + 1 = \\frac{504 - 2 + 2 \\cdot 0}{2} + 1 = 252$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360071c",
   "metadata": {},
   "source": [
    "### Task 1f)\n",
    "\n",
    "$W_2 = \\frac{252 - 3 + 2 \\cdot 0}{1} + 1 = 250$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e0621",
   "metadata": {},
   "source": [
    "### Task 1g)\n",
    "\n",
    "Since the input is a square RGB image of size $32$, we have $3$ channels and thereby an input of size $32 \\times 32 \\times 3$. This means that the $5 \\times 5$ convolutional moving with a stride of $S = 1$ with padding $P = 2$ makes a new layer of the exact same size. \n",
    "\n",
    "The number of parameters in the first convolutional layer is given by the size of the kernel, which are the weights, the number of channels pluss the bias. Times this with the number of filters and we have the number of parameters. The number of parameters in the first layer is thereby given by:\n",
    "\n",
    "$((5 \\cdot 5 \\cdot 3) + 1 ) \\cdot 32 = 2,432$\n",
    "\n",
    "Since the Max-Pooling layer only shrinks the dimension of a channel itself, but not the number of feature maps, the pooling layer has no effect on the amound of parameters in the network. \n",
    "\n",
    "To find the number of parameters for the next layers, we can use the same $((F \\cdot F \\cdot D_1) + b) \\cdot D_2 = N$ formula as above, where $F$ is the size of the kernel, $b$ is the number of biases, $D_1$ is the number of features in the previous layer and $D_2$ is the number of features in the next layer. For the two next convolutional layers this is:\n",
    "\n",
    "$((5 \\cdot 5 \\cdot 32) + 1) \\cdot 64 = 51,264$\n",
    "\n",
    "$((5 \\cdot 5 \\cdot 64) + 1) \\cdot 128 = 204,928$\n",
    "\n",
    "The width of the input channels is $W_i = 32$ and is halfed with each pooling layer, when it's time to flatten after the last pooling, the dimensions of each feature map is $4 \\times 4$. This yields:\n",
    "\n",
    "$((4 \\cdot 4 \\cdot 128) + 1) \\cdot 64 = 131,136$\n",
    "\n",
    "The number of parameters in the last layer is simply given by:\n",
    "\n",
    "$(64 + 1) \\cdot 10 = 650$\n",
    "\n",
    "This gives a total parameters of: \n",
    "\n",
    "$2,432 + 51,264 + 204928 + 131,136 + 650 = 390,410$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a176581",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaee0bd",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "\n",
    "Early stopping kicks in before epoch 10 and we reach a validation and test accuracy north of 70%, which we regard as pretty good given that we have not started tuning or messing with different techniques for making the CNN better.\n",
    "\n",
    "![](2a_fig_with_validation_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af63ce4",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "\n",
    "Final validation and test accuracies are 73.4% and 72.9%, respectivley.\n",
    "\n",
    "![](2b_snipp.png)\n",
    "\n",
    "![](2b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c1324",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839ef48",
   "metadata": {},
   "source": [
    "### Task 3a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f7d57",
   "metadata": {},
   "source": [
    "#### Model 1\n",
    "\n",
    "The architecture of the first model is given by the table 1. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Table 1: Model 1 }\\\\\n",
    "&\\begin{array}{llcc}\n",
    "\\hline \\hline \\text { Layer } & \\text { Layer type } & \\text { N } & \\text { Activation function } \\\\\n",
    "\\hline 1 & Conv2d & 32 & ReLU \\\\\n",
    "1 & BatchNorm2d & 32 & - \\\\\n",
    "1 & MaxPool2d & - & - \\\\\n",
    "2 & Conv2d & 64 & ReLU \\\\\n",
    "2 & MaxPool2d & - & - \\\\\n",
    "- & Flatten & - & - \\\\\n",
    "3 & Fully Connected & 64 & ReLU \\\\\n",
    "3 & BatchNorm2d & 64 & - \\\\\n",
    "4 & Fully Connected & 10 & Softmax \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Small architecture with a goal of trying to get as much as possible out of a small a network as possible.\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "Learning rate: $10^{-2}$\n",
    "\n",
    "Betas: Default\n",
    "\n",
    "Without L2 regularization (meaning no specified weight decay)\n",
    "\n",
    "Batch size: $64$\n",
    "\n",
    "Used a RandomHorizontalFlip with flipping 30% of the training images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f87dc",
   "metadata": {},
   "source": [
    "#### Model 2\n",
    "\n",
    "Model architecture:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Table 2: Model 2 }\\\\\n",
    "&\\begin{array}{llcc}\n",
    "\\hline \\hline \\text { Layer } & \\text { Layer type } & \\text { N } & \\text { Activation function } \\\\\n",
    "\\hline 1 & \\text{Conv2d} & 32 & \\text{ReLU} \\\\\n",
    "1 & \\text{BatchNorm2d} & 32 & - \\\\\n",
    "2 & \\text{Conv2d} & 64 & \\text{ReLU} \\\\\n",
    "2 & \\text{BatchNorm2d} & 64 & - \\\\\n",
    "2 & \\text{MaxPool2d} & - & - \\\\\n",
    "3 & \\text{Conv2d} & 128 & \\text{ReLU} \\\\\n",
    "3 & \\text{BatchNorm2d} & 128 & - \\\\\n",
    "4 & \\text{Conv2d} & 256 & \\text{ReLU} \\\\\n",
    "4 & \\text{MaxPool2d} & - & - \\\\\n",
    "5 & \\text{Conv2d} & 512 & \\text{ReLU} \\\\\n",
    "- & \\text{Flatten} & - & - \\\\\n",
    "6 & \\text{Fully Connected} & 512 \\cdot 8 \\cdot 8 & \\text{ReLU} \\\\\n",
    "6 & \\text{BatchNorm2d} & 64 & - \\\\\n",
    "7 & \\text{Fully Connected} & 10 & \\text{Softmax} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Goal of making a complex network that is precise.\n",
    "\n",
    "Optimizer: Adam \n",
    "\n",
    "Learning rate: $10^{-2}$\n",
    "Betas: Default\n",
    "\n",
    "Without L2 regulatiztion\n",
    "\n",
    "Batch size: $64$\n",
    "\n",
    "Used a RandomHorizontalFlip with flipping 30% of the training images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df905b58",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Table 2: Model comparisons }\\\\\n",
    "&\\begin{array}{ccccc}\n",
    "\\hline \\hline \\text{Model} & \\text { Validation Loss } & \\text { Validation Accuracy } & \\text { Test Loss } & \\text { Test Accuracy } \\\\ \n",
    "\\hline 1 & 0.65 & 0.786 & 0.68 & 0.779\\\\\n",
    "2 & 0.46 & 0.856 & 0.50 & 0.845 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "![](plots/task3_m2_adam_0837.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a8148",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "\n",
    "**What worked:**\n",
    "The thing that worked best was simply changing the architecture from something relativley small to a bigger one. Without doing any batch normalization, flip transforms or changing optimizers we got over 80% with out second net. This could be because, if there are more learnable weights, this ads complexity in how many things can be \"recognized\" in a given picture. The deeper feature extractior is A LOT slower when training though, because there are so many parameters that need to be updated per epoch. \n",
    "\n",
    "Batch normalization worked well, but best in the classifier, especially on the smaller network. This would be because the values of the weights will be smaller. This also means that it is faster to train given that smaller changes to each weight can be given.\n",
    "\n",
    "Doing the flip transform did wonders in both networks, radomizing and altering the input data, makes for better generalization and therefore better.\n",
    "\n",
    "Tweeking the learning rate and momentum parameters in SGD worked well, this is just about finding the right hyperparameters for the given network.\n",
    "\n",
    "In the end Adam was a better option for optimizer with all default hyperparameters.\n",
    "\n",
    "Changing kernel size to a smaller one in some of the later Conv layers was successfull, but only marginally in our case. Although other people we have spoken to have had bigger improvements doing this.\n",
    "\n",
    "**What didn't work:**\n",
    "Changing to a smaller kernel size in the first Conv layer was not very successfull, probably because it is better to have more learnable weights early in the feature extraction.\n",
    "\n",
    "Batch normalization after every Conv layer in our second model was a disaster, we have no idea why, but we ended up with a test accuracy of 0.100.\n",
    "\n",
    "Strided Convolution did worse than MaxPooling, we think this is the fact that MaxPooling takes out the most activated parts of a conv layer, while the strided convolutions simply applies the filters in a broader manner and might even skip important details in the layer.\n",
    "\n",
    "We tried using the Sigmoid Linear Unit (SiLU) activation function on the smaller model, but that did not work better than ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916aaa7",
   "metadata": {},
   "source": [
    "### Task 3d)\n",
    "\n",
    "It is clear by the plots below that the complexity of the architecture had most impact as these two models are very similar, but model 2 has a deeper architecture than model 1. They use the same optimizer, learning rate and data transforms.\n",
    "\n",
    "![](plots/task3_comparing_models_flipped_images_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be08bb",
   "metadata": {},
   "source": [
    "### Task 3e)\n",
    "\n",
    "This can be seen in the plots in both **3b)** and **3d)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f2611",
   "metadata": {},
   "source": [
    "### Task 3f)\n",
    "\n",
    "We can by the last plot see that out models do not overfit as we see that validation loss for the two models do not increas before early stopping. They do not overfit either, because then they would be in free fall downwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a57b9",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c91792",
   "metadata": {},
   "source": [
    "### Task 4a)\n",
    "\n",
    "Optimizer: Adam \n",
    "Batch size: 32\n",
    "Learning rate: $5 \\cdot 10^{-4}$\n",
    "Data augmentation: Normalization and Resize\n",
    "\n",
    "![](plots/4a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f0791",
   "metadata": {},
   "source": [
    "### Task 4b)\n",
    "\n",
    "We can see that the activation spots some edges as well as contrasts.\n",
    "\n",
    "![](plots/4b.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
